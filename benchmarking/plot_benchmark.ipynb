{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_df = pd.read_csv('data/cox_inhibitor.tsv', sep='\\t')\n",
    "cox_inhibitors = ['-'.join(x.upper().split(' '))  for x in cox_df['Drug'].values]\n",
    "hdac_df = pd.read_csv('data/hdac_inhibitors.csv', index_col=0)\n",
    "hdac_inhibitors = ['-'.join(x.upper().split(' ')) for x in hdac_df['Drug'].values]\n",
    "cdk_df = pd.read_csv('data/CDK inhibitor.txt', sep='\\t')\n",
    "cdk_inhibitors = ['-'.join(x.upper().split(' ')) for x in cdk_df['Name'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_dict = {\n",
    "    'pvalue': {'scores': [], 'labels': []},\n",
    "    'oddsRatio': {'scores': [], 'labels': []}, \n",
    "    'pvalueUp': {'scores': [], 'labels': []}, \n",
    "    'pvalueDown': {'scores': [], 'labels': []}, \n",
    "    'oddsRatioUp': {'scores': [], 'labels': []},\n",
    "    'oddsRatioDown': {'scores': [], 'labels': []}\n",
    "}\n",
    "\n",
    "for metric in ranking_dict.keys():\n",
    "    for term in tqdm(os.listdir('data/hdac_out')):\n",
    "        rank_df = pd.read_csv(f'data/hdac_out/{term}', sep='\\t', index_col=0)\n",
    "        rank_df = rank_df[(rank_df['pvalue'] < 0.01)]\n",
    "        if 'pvalue' in metric:\n",
    "            rank_df.sort_values(by=metric, inplace=True, ascending=True)\n",
    "        else:\n",
    "            rank_df.sort_values(by=metric, inplace=True, ascending=False)\n",
    "        rank_df.reset_index(drop=True, inplace=True)\n",
    "        rank_df['labels'] = [1 if x.upper() in hdac_inhibitors else 0 for x in rank_df['drug']]\n",
    "        rank_df['scores'] = 1 -  ((rank_df.index.values) / len(rank_df))\n",
    "        ranking_dict[metric]['scores'].extend(list(rank_df['scores']))\n",
    "        ranking_dict[metric]['labels'].extend(list(rank_df['labels']))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for metric in ranking_dict.keys():   \n",
    "    fpr, tpr, thresholds = roc_curve(ranking_dict[metric]['labels'], ranking_dict[metric]['scores'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{metric} (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random')\n",
    "plt.savefig('data/figures/adj_pvalue_hdac.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ranking_dict_cdk = {\n",
    "    'pvalue': {'scores': [], 'labels': []},\n",
    "    'oddsRatio': {'scores': [], 'labels': []}, \n",
    "    'pvalueUp': {'scores': [], 'labels': []}, \n",
    "    'pvalueDown': {'scores': [], 'labels': []}, \n",
    "    'oddsRatioUp': {'scores': [], 'labels': []},\n",
    "    'oddsRatioDown': {'scores': [], 'labels': []}\n",
    "}\n",
    "\n",
    "for metric in ranking_dict_cdk.keys():\n",
    "    for term in tqdm(os.listdir('data/cdk_out')):\n",
    "        rank_df = pd.read_csv(f'data/cdk_out/{term}', sep='\\t', index_col=0)\n",
    "        rank_df = rank_df[(rank_df['pvalue'] < 0.01)]\n",
    "        if 'pvalue' in metric:\n",
    "            rank_df.sort_values(by=metric, inplace=True, ascending=True)\n",
    "        else:\n",
    "            rank_df.sort_values(by=metric, inplace=True, ascending=False)\n",
    "        rank_df.reset_index(drop=True, inplace=True)\n",
    "        rank_df['labels'] = [1 if x.upper() in cdk_inhibitors else 0 for x in rank_df['drug']]\n",
    "        rank_df['scores'] = 1 -  ((rank_df.index.values) / len(rank_df))\n",
    "        ranking_dict_cdk[metric]['scores'].extend(list(rank_df['scores']))\n",
    "        ranking_dict_cdk[metric]['labels'].extend(list(rank_df['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for metric in ranking_dict_cdk.keys():   \n",
    "    fpr, tpr, thresholds = roc_curve(ranking_dict_cdk[metric]['labels'], ranking_dict_cdk[metric]['scores'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{metric} (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "#plt.xlim([0, 0.2])\n",
    "#plt.ylim([0, .4])\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_roc_curve(ones: list, zeros: list, n: int):\n",
    "    \"\"\"\n",
    "    Calculate the mean area under the ROC curve (AUC) and the mean true positive rate (TPR)\n",
    "    using bootstrap resampling.\n",
    "\n",
    "    Parameters:\n",
    "    ones (array-like): The positive class labels.\n",
    "    zeros (array-like): The negative class labels.\n",
    "    n (int): The number of bootstrap iterations.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the mean AUC and the mean TPR.\n",
    "\n",
    "    \"\"\"\n",
    "    base_fpr = np.linspace(0, 1, 50)\n",
    "    size_group1 = len(ones)\n",
    "    sum_auc = 0\n",
    "    tprs = []\n",
    "    for i in range(n):\n",
    "        zeros_sampled = np.random.choice(zeros, size=size_group1, replace=True)\n",
    "        fpr, tpr, _ = roc_curve(np.concatenate([np.ones_like(ones), np.zeros_like(zeros_sampled)]),\n",
    "                                np.concatenate([ones, zeros_sampled]), drop_intermediate=False)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        sum_auc += roc_auc\n",
    "        tpr = np.interp(base_fpr, fpr, tpr)\n",
    "        tpr[0] = 0.0\n",
    "        tprs.append(list(tpr))\n",
    "\n",
    "    auc_mean = sum_auc / n\n",
    "\n",
    "    tpr_mean = np.mean(np.array(tprs), axis=0)\n",
    "    return {'auc': auc_mean, 'approx': tpr_mean}\n",
    "\n",
    "\n",
    "def plot_roc_curve(roc_vals: dict, name: str, n_bootstrap=5000):\n",
    "    \"\"\"\n",
    "    Plots the ROC curve for the given ROC values.\n",
    "\n",
    "    Parameters:\n",
    "    roc_vals (dict): Dictionary containing the ROC values for different libraries.\n",
    "    name (str): name of output figure\n",
    "    n_bootstrap (int): Number of bootstrap iterations for calculating confidence intervals. Default is 5000.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    base_fpr = np.linspace(0, 1, 50)\n",
    "\n",
    "    lib_curves = {}\n",
    "    for lib in tqdm(roc_vals):\n",
    "        scores = roc_vals[lib]['scores']\n",
    "        labels = np.array(roc_vals[lib]['labels'])\n",
    "        idx1 = np.where(labels == 1)\n",
    "        idx0 = np.where(labels == 0)\n",
    "        tp = np.array(scores)[idx1]\n",
    "        fp = np.array(scores)[idx0]\n",
    "        bootstrapped_res = bootstrap_roc_curve(tp, fp, n_bootstrap)\n",
    "        print(lib, bootstrapped_res['auc'])\n",
    "        lib_curves[lib] = bootstrapped_res\n",
    "\n",
    "    # Plotting\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    for i, lib in enumerate(list(lib_curves)):\n",
    "        plt.plot(base_fpr, lib_curves[lib]['approx'], label=f\"{lib} AUC: {np.round(lib_curves[lib]['auc'], 3)}\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(ranking_dict, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crispr_hdac_ranks = {\n",
    "    'pvalue': {'scores': [], 'labels': []},\n",
    "    'oddsRatio': {'scores': [], 'labels': []},\n",
    "    'pvalueUp': {'scores': [], 'labels': []}, \n",
    "    'pvalueDown': {'scores': [], 'labels': []}, \n",
    "    'oddsRatioUp': {'scores': [], 'labels': []},\n",
    "    'oddsRatioDown': {'scores': [], 'labels': []}\n",
    "}\n",
    "\n",
    "for metric in crispr_hdac_ranks.keys():\n",
    "    for term in tqdm(os.listdir('data/hdac_out')):\n",
    "        rank_df = pd.read_csv(f'data/hdac_out/{term}', sep='\\t', index_col=0)   \n",
    "        rank_df = rank_df[rank_df['drug'].str.contains(' ')]\n",
    "        rank_df = rank_df[(rank_df['pvalue'] < 0.05)]\n",
    "        if 'pvalue' in metric:\n",
    "            rank_df.sort_values(by=metric, inplace=True, ascending=True)\n",
    "        else:\n",
    "            rank_df.sort_values(by=metric, inplace=True, ascending=False)\n",
    "        rank_df.reset_index(drop=True, inplace=True)\n",
    "        rank_df['labels'] = [1 if 'HDAC' in x.upper() else 0 for x in rank_df['drug']]\n",
    "        rank_df['scores'] = 1 -  ((rank_df.index.values) / len(rank_df))\n",
    "        crispr_hdac_ranks[metric]['scores'].extend(list(rank_df['scores']))\n",
    "        crispr_hdac_ranks[metric]['labels'].extend(list(rank_df['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_genes = {}\n",
    "\n",
    "for term in tqdm(os.listdir('data/hdac_out')):\n",
    "    rank_df = pd.read_csv(f'data/hdac_out/{term}', sep='\\t', index_col=0)\n",
    "    rank_df = rank_df[rank_df['drug'].str.contains(' ')]\n",
    "    rank_df.reset_index(drop=True, inplace=True)\n",
    "    rank_df['score'] = 1 -  ((rank_df.index.values) / len(rank_df))\n",
    "    rank_df = rank_df[(rank_df['pvalue'] < 0.05)]\n",
    "    for pert in rank_df['drug'].unique():\n",
    "        if pert not in score_genes:\n",
    "            score_genes[pert] = []\n",
    "        pert_df = rank_df[rank_df['drug'] == pert]\n",
    "        score_genes[pert].extend(list(pert_df['score'].values))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = []\n",
    "for g in score_genes:\n",
    "    gene_data.append([g, np.mean(score_genes[g]), np.std(score_genes[g]), len(score_genes[g])])\n",
    "gene_hdac_df = pd.DataFrame(gene_data, columns=['gene', 'mean', 'std', 'num appearances']).sort_values(by='mean', ascending=False)\n",
    "gene_hdac_df.set_index('gene', inplace=True)\n",
    "n = len(os.listdir('data/hdac_out'))\n",
    "gene_hdac_df['percent appearances'] = ((gene_hdac_df['num appearances'] / n) * 100).round(2)\n",
    "display(gene_hdac_df[ gene_hdac_df.index.str.contains('HDAC')].sort_values(by='num appearances', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_genes_cdk = {}\n",
    "\n",
    "for term in tqdm(os.listdir('data/cdk_out')):\n",
    "    rank_df = pd.read_csv(f'data/cdk_out/{term}', sep='\\t', index_col=0)\n",
    "    rank_df = rank_df[rank_df['drug'].str.contains(' ')]\n",
    "    rank_df.reset_index(drop=True, inplace=True)\n",
    "    rank_df['score'] = 1 -  ((rank_df.index.values) / len(rank_df))\n",
    "    rank_df = rank_df[(rank_df['pvalue'] < 0.05)]\n",
    "    for pert in rank_df['drug'].unique():\n",
    "        if pert not in score_genes_cdk:\n",
    "            score_genes_cdk[pert] = []\n",
    "        pert_df = rank_df[rank_df['drug'] == pert]\n",
    "        score_genes_cdk[pert].extend(list(pert_df['score'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = []\n",
    "for g in score_genes_cdk:\n",
    "    gene_data.append([g, np.mean(score_genes_cdk[g]), np.std(score_genes_cdk[g]), len(score_genes_cdk[g])])\n",
    "gene_cdk_df = pd.DataFrame(gene_data, columns=['gene', 'mean scaled rank', 'std', 'num appearances']).sort_values(by='mean scaled rank', ascending=False)\n",
    "gene_cdk_df.set_index('gene', inplace=True)\n",
    "n = len(os.listdir('data/cdk_out'))\n",
    "gene_cdk_df['percent appearances'] = ((gene_cdk_df['num appearances'] / n) * 100).round(2)\n",
    "display(gene_cdk_df[ gene_cdk_df.index.str.contains('CDK')].sort_values(by='num appearances', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_cdk_df.sort_values(by='num appearances', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gene_hdac_df[ (gene_hdac_df['num appearances'] > 10) & (gene_hdac_df['mean'] > 0.9)]\n",
    "#gene_hdac_df[ (gene_hdac_df['num appearances'] > 20) & (gene_hdac_df['mean'] > 0.95)]\n",
    "top_hdac = set(gene_hdac_df.sort_values(by='num appearances', ascending=False).index.values[:250])\n",
    "top_cdk  = set(gene_cdk_df.sort_values(by='num appearances', ascending=False).index.values[:250])\n",
    "diff_hdac = top_hdac.difference(top_cdk)\n",
    "diff_cdk = top_cdk.difference(top_hdac)\n",
    "intersection = top_hdac.intersection(top_cdk)\n",
    "\n",
    "import matplotlib_venn\n",
    "matplotlib_venn.venn2([top_hdac, top_cdk], set_labels=('HDAC top 250 KOs', 'CDK top 250 KOs'))\n",
    "plt.savefig('venn_hdac_cdk.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdk_unique = [g for g in top_cdk if g not in top_hdac]\n",
    "hdac_unique = [g for g in top_hdac if g not in top_cdk]\n",
    "len(cdk_unique), len(hdac_unique), len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hdac_unique.txt', 'w') as f:\n",
    "    f.write('\\n'.join(hdac_unique))\n",
    "\n",
    "with open('cdk_unique.txt', 'w') as f:\n",
    "    f.write('\\n'.join(cdk_unique))\n",
    "\n",
    "with open('intersection.txt', 'w') as f:\n",
    "    f.write('\\n'.join(intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for metric in crispr_hdac_ranks.keys():   \n",
    "    fpr, tpr, thresholds = roc_curve(crispr_hdac_ranks[metric]['labels'], crispr_hdac_ranks[metric]['scores'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{metric} (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crispr_cdk_ranks = {\n",
    "    'pvalue': {'scores': [], 'labels': []},\n",
    "    'oddsRatio': {'scores': [], 'labels': []},\n",
    "    'pvalueUp': {'scores': [], 'labels': []}, \n",
    "    'pvalueDown': {'scores': [], 'labels': []}, \n",
    "    'oddsRatioUp': {'scores': [], 'labels': []},\n",
    "    'oddsRatioDown': {'scores': [], 'labels': []}\n",
    "}\n",
    "\n",
    "for metric in crispr_cdk_ranks.keys():\n",
    "    for term in tqdm(os.listdir('data/cdk_out')):\n",
    "        rank_df = pd.read_csv(f'data/cdk_out/{term}', sep='\\t', index_col=0)   \n",
    "        rank_df = rank_df[rank_df['drug'].str.contains(' ')]\n",
    "        rank_df = rank_df[(rank_df['pvalue'] < 0.05)]\n",
    "        if 'pvalue' in metric:\n",
    "            rank_df.sort_values(by=metric, inplace=True, ascending=True)\n",
    "        else:\n",
    "            rank_df.sort_values(by=metric, inplace=True, ascending=False)\n",
    "        rank_df.reset_index(drop=True, inplace=True)\n",
    "        rank_df['labels'] = [1 if 'CDK' in x.upper() else 0 for x in rank_df['drug']]\n",
    "        rank_df['scores'] = 1 -  ((rank_df.index.values) / len(rank_df))\n",
    "        crispr_cdk_ranks[metric]['scores'].extend(list(rank_df['scores']))\n",
    "        crispr_cdk_ranks[metric]['labels'].extend(list(rank_df['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for metric in crispr_cdk_ranks.keys():   \n",
    "    fpr, tpr, thresholds = roc_curve(crispr_cdk_ranks[metric]['labels'], crispr_cdk_ranks[metric]['scores'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{metric} (AUC = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(alpha=0.3)\n",
    "plt.ylim([0, .4])\n",
    "plt.xlim([0, 0.2])\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdac_term_search = pd.read_csv('data/results (10).tsv', sep='\\t')\n",
    "cdk_term_search = pd.read_csv('data/results (9).tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdac_term_search['KO'] = hdac_term_search['term'].map(lambda x: x.split('_')[4])\n",
    "hdac_term_search = hdac_term_search[hdac_term_search['KO'].str.contains(' ')]\n",
    "hdac_term_search['KO'] = hdac_term_search['KO'].map(lambda x: x.split(' ')[0])\n",
    "hdac_kos = hdac_term_search['KO'].values\n",
    "set(hdac_kos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdk_term_search['KO'] = cdk_term_search['term'].map(lambda x: x.split('_')[4])\n",
    "cdk_term_search = cdk_term_search[cdk_term_search['KO'].str.contains(' ')]\n",
    "cdk_term_search['KO'] = cdk_term_search['KO'].map(lambda x: x.split(' ')[0])\n",
    "cdk_kos = cdk_term_search['KO'].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
